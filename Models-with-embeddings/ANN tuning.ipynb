{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197c46b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score, accuracy_score\n",
    "import mlflow\n",
    "from mlflow.models.signature import infer_signature\n",
    "import numpy as np\n",
    "\n",
    "import keras_tuner as kt\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- NN Configuration ---\n",
    "# We'll use these for the baseline experiments\n",
    "NN_EPOCHS = 100\n",
    "NN_BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8c5d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- File Paths ---\n",
    "SOURCE_DATASET_PATH = \"data/final_dataset_with_embeddings.csv\" # Your final dataset with embeddings\n",
    "BASE_LOG_DIR = \"logs\" # A parent directory to store all results\n",
    "\n",
    "# --- Feature Configuration ---\n",
    "METADATA_COLS = [\"commit_hash\", \"author_email\", \"commit_date\"]\n",
    "LABEL_COL = \"is_bug_introducing\"\n",
    "N_PCA_COMPONENTS = 177 # The optimal number you found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b82856e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_base_data(path):\n",
    "    \"\"\"Loads the source CSV, cleans it, and sorts by date.\"\"\"\n",
    "    print(f\"Loading and preparing base data from '{path}'...\")\n",
    "    df = pd.read_csv(path)\n",
    "    df.dropna(subset=['commit_hash', LABEL_COL], inplace=True)\n",
    "    df[\"commit_date\"] = pd.to_datetime(df[\"commit_date\"])\n",
    "    df.sort_values(by=\"commit_date\", inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    print(\"Base data loaded successfully.\")\n",
    "    return df\n",
    "\n",
    "def prepare_feature_sets(df):\n",
    "    \"\"\"Creates all the different feature combinations for our experiments.\"\"\"\n",
    "    print(\"Preparing all feature sets...\")\n",
    "    \n",
    "    embedding_cols = [col for col in df.columns if col.startswith('emb_')]\n",
    "    stats_cols = [col for col in df.columns if col not in embedding_cols + METADATA_COLS + [LABEL_COL]]\n",
    "    \n",
    "    # Normalize and apply PCA to embeddings\n",
    "    X_embed = df[embedding_cols].values\n",
    "    X_normalized = Normalizer(norm='l2').fit_transform(X_embed)\n",
    "    pca = PCA(n_components=N_PCA_COMPONENTS, random_state=42)\n",
    "    X_pca = pca.fit_transform(X_normalized)\n",
    "    \n",
    "    pca_cols = [f'pca_{i+1}' for i in range(N_PCA_COMPONENTS)]\n",
    "    df_pca = pd.DataFrame(X_pca, columns=pca_cols)\n",
    "\n",
    "    feature_sets = {\n",
    "        \"stats_only\": df[stats_cols],\n",
    "        \"embeddings_only\": df[embedding_cols],\n",
    "        \"pca_only\": df_pca,\n",
    "        \"stats_and_embeddings\": pd.concat([df[stats_cols], df[embedding_cols]], axis=1),\n",
    "        \"stats_and_pca\": pd.concat([df[stats_cols], df_pca], axis=1)\n",
    "    }\n",
    "    \n",
    "    print(\"All feature sets are ready.\")\n",
    "    return feature_sets, df[LABEL_COL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df46862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tunable_model(hp):\n",
    "    \"\"\"Builds a Keras model with tunable hyperparameters.\"\"\"\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # Tune the number of hidden layers (1 to 3)\n",
    "    for i in range(hp.Int('num_layers', 1, 3)):\n",
    "        # Tune the number of neurons in each layer\n",
    "        model.add(layers.Dense(\n",
    "            units=hp.Int(f'units_{i}', min_value=32, max_value=256, step=32),\n",
    "            activation='relu'\n",
    "        ))\n",
    "        # Tune the dropout rate for each layer\n",
    "        model.add(layers.Dropout(\n",
    "            rate=hp.Float(f'dropout_{i}', min_value=0.2, max_value=0.5, step=0.1)\n",
    "        ))\n",
    "        \n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Tune the learning rate\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy'] # Objective is specified in the tuner\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b860ca99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Select your best performing feature set from the baseline experiments ---\n",
    "best_feature_set_name = \"NN_Stats_and_PCA\" # Change this based on your results\n",
    "X_data_best = all_feature_sets[best_feature_set_name.replace(\"NN_\", \"\").lower()]\n",
    "\n",
    "# Split and scale the best data\n",
    "split_point = int(len(X_data_best) * 0.80)\n",
    "X_train, X_test = X_data_best.iloc[:split_point], X_data_best.iloc[split_point:]\n",
    "y_train, y_test = y_data.iloc[:split_point], y_data.iloc[split_point:]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- Set up the KerasTuner ---\n",
    "tuner = kt.Hyperband(\n",
    "    build_tunable_model,\n",
    "    objective='val_loss', # Objective to minimize\n",
    "    max_epochs=30,\n",
    "    factor=3,\n",
    "    directory=os.path.join(BASE_LOG_DIR, 'keras_tuner'),\n",
    "    project_name=f'tune_{best_feature_set_name}'\n",
    ")\n",
    "\n",
    "# Define a callback to stop training early if the model is not improving\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "print(f\"\\n--- Starting Hyperparameter and Architecture Search for '{best_feature_set_name}' ---\")\n",
    "tuner.search(\n",
    "    X_train_scaled, \n",
    "    y_train, \n",
    "    epochs=50, \n",
    "    validation_data=(X_test_scaled, y_test), \n",
    "    callbacks=[stop_early]\n",
    ")\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"\\n--- Search Complete --- âœ…\")\n",
    "print(f\"Optimal number of layers: {best_hps.get('num_layers')}\")\n",
    "print(f\"Optimal learning rate: {best_hps.get('learning_rate')}\")\n",
    "# You can print other best params similarly...\n",
    "\n",
    "# Build the best model and train it on the full data\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "history = best_model.fit(X_train_scaled, y_train, epochs=50, validation_data=(X_test_scaled, y_test))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

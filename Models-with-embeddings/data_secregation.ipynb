{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f69e539",
   "metadata": {},
   "source": [
    "# Data Segregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccd5b16",
   "metadata": {},
   "source": [
    "by seperating large diff and small diff we can handle memory load and speed using deffernt approach in tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4d7445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import sys\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86375c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "LARGE_CSV_PATH = \"data/final_dataset_with_diffs.csv\"\n",
    "SMALL_DIFFS_PATH = \"data/small_diffs.csv\"\n",
    "LARGE_DIFFS_PATH = \"data/large_diffs.csv\"\n",
    "NUL_BYTE_LOG_PATH = \"log/nul_byte_lines.txt\" # File to log line numbers of skipped rows\n",
    "SIZE_LIMIT_MB = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "383947b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segregating 'final_dataset_with_diffs.csv'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a30987da9148088e8d335ca605e525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scanning Rows: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Segregation Complete --- ✅\n",
      "Wrote 158890 rows to 'small_diffs.csv'\n",
      "Wrote 0 rows to 'large_diffs.csv'\n",
      "Skipped 1186 problematic rows containing NUL bytes. See 'nul_byte_lines.txt'.\n"
     ]
    }
   ],
   "source": [
    "# --- Setup ---\n",
    "size_limit_bytes = SIZE_LIMIT_MB * 1024 * 1024\n",
    "# Increase the CSV field size limit to handle large diffs\n",
    "max_int = sys.maxsize\n",
    "while True:\n",
    "    try:\n",
    "        csv.field_size_limit(max_int)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        max_int = int(max_int / 10)\n",
    "\n",
    "# --- Main Segregation Loop ---\n",
    "print(f\"Segregating '{LARGE_CSV_PATH}'...\")\n",
    "\n",
    "small_count = 0\n",
    "large_count = 0\n",
    "nul_count = 0\n",
    "\n",
    "try:\n",
    "    with open(LARGE_CSV_PATH, 'r', encoding='utf-8') as infile, \\\n",
    "         open(SMALL_DIFFS_PATH, 'w', newline='', encoding='utf-8') as small_outfile, \\\n",
    "         open(LARGE_DIFFS_PATH, 'w', newline='', encoding='utf-8') as large_outfile, \\\n",
    "         open(NUL_BYTE_LOG_PATH, 'w', encoding='utf-8') as nul_log_file:\n",
    "\n",
    "        # Read header and create writers\n",
    "        header_line = infile.readline()\n",
    "        header = header_line.strip().split(',')\n",
    "        \n",
    "        small_writer = csv.writer(small_outfile)\n",
    "        large_writer = csv.writer(large_outfile)\n",
    "        small_writer.writerow(header)\n",
    "        large_writer.writerow(header)\n",
    "        \n",
    "        diff_col_index = header.index('diff')\n",
    "\n",
    "        # Iterate through the rest of the file, tracking line numbers\n",
    "        for line_num, line in enumerate(tqdm(infile, desc=\"Scanning Rows\"), start=2):\n",
    "            \n",
    "            # --- THE KEY CHANGE IS HERE ---\n",
    "            # Check for NULL bytes before doing anything else\n",
    "            if '\\x00' in line:\n",
    "                nul_log_file.write(f\"{line_num}\\n\") # Log the line number\n",
    "                nul_count += 1\n",
    "                continue # Skip this row and continue to the next\n",
    "            \n",
    "            # If the line is clean, parse it\n",
    "            try:\n",
    "                parsed_row = next(csv.reader([line]))\n",
    "                if len(parsed_row) != len(header):\n",
    "                    continue # Skip rows with column mismatches\n",
    "\n",
    "                diff_text = parsed_row[diff_col_index]\n",
    "                \n",
    "                # Perform segregation logic\n",
    "                if len(diff_text.encode('utf-8', 'ignore')) > size_limit_bytes:\n",
    "                    large_writer.writerow(parsed_row)\n",
    "                    large_count += 1\n",
    "                else:\n",
    "                    small_writer.writerow(parsed_row)\n",
    "                    small_count += 1\n",
    "            except Exception:\n",
    "                # Catch any other parsing errors on non-NUL lines\n",
    "                nul_log_file.write(f\"{line_num}\\n\")\n",
    "                nul_count += 1\n",
    "                continue\n",
    "\n",
    "    print(\"\\n--- Segregation Complete --- ✅\")\n",
    "    print(f\"Wrote {small_count} rows to '{SMALL_DIFFS_PATH}'\")\n",
    "    print(f\"Wrote {large_count} rows to '{LARGE_DIFFS_PATH}'\")\n",
    "    if nul_count > 0:\n",
    "        print(f\"Skipped {nul_count} problematic rows containing NUL bytes. See '{NUL_BYTE_LOG_PATH}'.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Input file not found at '{LARGE_CSV_PATH}'\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d509c5",
   "metadata": {},
   "source": [
    "❌ failed this script cant handle the nul_bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe75d03b",
   "metadata": {},
   "source": [
    "### dataset cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf766983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import sys\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "LARGE_CSV_PATH = \"data/final_dataset_with_diffs.csv\"\n",
    "CLEAN_CSV_PATH = \"data/final_dataset_with_full_diffs_CLEANED.csv\"\n",
    "NUL_COMMIT_LOG_PATH = \"log/nul_commit_hashes.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e87df257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to clean 'final_dataset_with_diffs.csv' and log problematic commits...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8204b228be334b018cd93ffb76715909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scanning and Cleaning: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cleaning Complete --- ✅\n",
      "Clean file saved to 'final_dataset_with_full_diffs_CLEANED.csv'.\n",
      "Found and cleaned 898 rows containing NULL bytes.\n",
      "The commit hashes of these rows have been attempted to be saved to 'nul_commit_hashes.log'.\n"
     ]
    }
   ],
   "source": [
    "# --- Setup ---\n",
    "max_int = sys.maxsize\n",
    "while True:\n",
    "    try:\n",
    "        csv.field_size_limit(max_int)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        max_int = int(max_int / 10)\n",
    "\n",
    "# --- Main Segregation Loop with Improved Error Logging ---\n",
    "print(f\"Starting to clean '{LARGE_CSV_PATH}' and log problematic commits...\")\n",
    "nul_rows_found = 0\n",
    "\n",
    "with open(LARGE_CSV_PATH, 'rb') as infile, \\\n",
    "     open(CLEAN_CSV_PATH, 'wb') as outfile, \\\n",
    "     open(NUL_COMMIT_LOG_PATH, 'w', encoding='utf-8') as logfile:\n",
    "\n",
    "    header_bytes = infile.readline()\n",
    "    outfile.write(header_bytes)\n",
    "    header_str = header_bytes.decode('utf-8', 'ignore').strip()\n",
    "    header_list = header_str.split(',')\n",
    "    \n",
    "    try:\n",
    "        commit_hash_index = header_list.index('commit_hash')\n",
    "    except ValueError:\n",
    "        print(\"ERROR: 'commit_hash' column not found in the header.\")\n",
    "        raise\n",
    "\n",
    "    for line_bytes in tqdm(infile, desc=\"Scanning and Cleaning\"):\n",
    "        if b'\\x00' in line_bytes:\n",
    "            nul_rows_found += 1\n",
    "            \n",
    "            # Decode the line, replacing bad characters\n",
    "            line_str = line_bytes.decode('utf-8', 'replace')\n",
    "            \n",
    "            # --- THE KEY CHANGE IS HERE ---\n",
    "            # For these broken lines, use a simple split instead of the strict csv.reader\n",
    "            try:\n",
    "                parts = line_str.split(',')\n",
    "                if len(parts) > commit_hash_index:\n",
    "                    commit_hash = parts[commit_hash_index]\n",
    "                    logfile.write(f\"{commit_hash}\\n\")\n",
    "                else:\n",
    "                    logfile.write(\"Unparseable_row_short_after_split\\n\")\n",
    "            except Exception:\n",
    "                logfile.write(\"Unparseable_row_hard_error\\n\")\n",
    "\n",
    "            # Clean the NULL byte and write the line to the output\n",
    "            cleaned_line_bytes = line_bytes.replace(b'\\x00', b'')\n",
    "            outfile.write(cleaned_line_bytes)\n",
    "        else:\n",
    "            # If the line is clean, write it directly\n",
    "            outfile.write(line_bytes)\n",
    "\n",
    "print(\"\\n--- Cleaning Complete --- ✅\")\n",
    "print(f\"Clean file saved to '{CLEAN_CSV_PATH}'.\")\n",
    "if nul_rows_found > 0:\n",
    "    print(f\"Found and cleaned {nul_rows_found} rows containing NULL bytes.\")\n",
    "    print(f\"The commit hashes of these rows have been attempted to be saved to '{NUL_COMMIT_LOG_PATH}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f32c4d",
   "metadata": {},
   "source": [
    "## Segregation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43cba109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "CLEAN_CSV_PATH = \"final_dataset_with_full_diffs_CLEANED.csv\"\n",
    "SMALL_DIFFS_PATH = \"small_diffs.csv\"\n",
    "LARGE_DIFFS_PATH = \"large_diffs.csv\"\n",
    "SIZE_LIMIT_MB = 10\n",
    "CHUNK_SIZE = 500 # Process 500 rows at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfb3000b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segregating 'final_dataset_with_full_diffs_CLEANED.csv' using pandas chunking...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e94ee0dfa7d546c7aa6369ef9b758bb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Chunks: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Segregation Complete --- ✅\n",
      "Small diffs saved to 'small_diffs.csv'\n",
      "Large diffs saved to 'large_diffs.csv'\n"
     ]
    }
   ],
   "source": [
    "# --- Setup ---\n",
    "size_limit_bytes = SIZE_LIMIT_MB * 1024 * 1024\n",
    "\n",
    "print(f\"Segregating '{CLEAN_CSV_PATH}' using pandas chunking...\")\n",
    "\n",
    "# Create the CSV reader which yields DataFrames (chunks)\n",
    "try:\n",
    "    csv_reader = pd.read_csv(CLEAN_CSV_PATH, chunksize=CHUNK_SIZE)\n",
    "\n",
    "    # Use tqdm to track progress over the chunks\n",
    "    for i, chunk_df in enumerate(tqdm(csv_reader, desc=\"Processing Chunks\")):\n",
    "        # Calculate the byte size of each diff in the chunk\n",
    "        diff_sizes = chunk_df['diff'].astype(str).str.encode('utf-8', 'ignore').str.len()\n",
    "\n",
    "        # Split the chunk into small and large diffs based on the size limit\n",
    "        large_df = chunk_df[diff_sizes > size_limit_bytes]\n",
    "        small_df = chunk_df[diff_sizes <= size_limit_bytes]\n",
    "        \n",
    "        # Append to the respective CSV files\n",
    "        # The header is only written for the very first chunk\n",
    "        header = i == 0\n",
    "        \n",
    "        if not small_df.empty:\n",
    "            small_df.to_csv(SMALL_DIFFS_PATH, mode='a', header=header, index=False)\n",
    "            \n",
    "        if not large_df.empty:\n",
    "            large_df.to_csv(LARGE_DIFFS_PATH, mode='a', header=header, index=False)\n",
    "\n",
    "    print(\"\\n--- Segregation Complete --- ✅\")\n",
    "    print(f\"Small diffs saved to '{SMALL_DIFFS_PATH}'\")\n",
    "    print(f\"Large diffs saved to '{LARGE_DIFFS_PATH}'\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Clean file not found at '{CLEAN_CSV_PATH}'. Please ensure the cleaning step was completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during segregation: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

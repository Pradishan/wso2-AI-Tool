{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0765a5a2",
   "metadata": {},
   "source": [
    "# Tokenizing small diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06beb156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12eb2b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "INPUT_CSV = \"../small_diffs.csv\"\n",
    "TOKENIZED_DATA_DIR = \"../tokenized_data_test/small_diffs\"\n",
    "CHUNK_SIZE = 500 # How many rows to process at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f7a1f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing small diffs: Resuming from chunk #0.\n"
     ]
    }
   ],
   "source": [
    "# --- Setup ---\n",
    "os.makedirs(TOKENIZED_DATA_DIR, exist_ok=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "# --- Checkpointing: Determine where to resume from ---\n",
    "processed_chunks = 0\n",
    "if os.path.exists(TOKENIZED_DATA_DIR):\n",
    "    existing_files = [f for f in os.listdir(TOKENIZED_DATA_DIR) if f.startswith('chunk_') and f.endswith('.pt')]\n",
    "    if existing_files:\n",
    "        processed_chunks = len(existing_files)\n",
    "\n",
    "start_chunk = processed_chunks\n",
    "rows_to_skip = start_chunk * CHUNK_SIZE\n",
    "print(f\"Processing small diffs: Resuming from chunk #{start_chunk}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f521fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e9b73f5e8764fbbb7448601396af592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Small Diffs: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Small diff processing complete. ✅\n"
     ]
    }
   ],
   "source": [
    "# --- Main Resumable Loop for Small Diffs ---\n",
    "try:\n",
    "    csv_reader = pd.read_csv(INPUT_CSV, chunksize=CHUNK_SIZE, skiprows=range(1, rows_to_skip + 1))\n",
    "    \n",
    "    total_rows_processed = 0\n",
    "    for i, chunk_df in enumerate(tqdm(csv_reader, desc=\"Processing Small Diffs\")):\n",
    "        current_chunk_index = start_chunk + i\n",
    "        \n",
    "        diff_texts = chunk_df['diff'].astype(str).tolist()\n",
    "        labels = chunk_df['is_bug_introducing'].tolist()\n",
    "        \n",
    "        encodings = tokenizer(diff_texts, truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n",
    "        \n",
    "        chunk_path = os.path.join(TOKENIZED_DATA_DIR, f\"chunk_{current_chunk_index}.pt\")\n",
    "        torch.save({\n",
    "            'input_ids': encodings['input_ids'],\n",
    "            'attention_mask': encodings['attention_mask'],\n",
    "            'labels': torch.tensor(labels)\n",
    "        }, chunk_path)\n",
    "        \n",
    "        total_rows_processed += len(chunk_df)\n",
    "        del diff_texts, labels, encodings, chunk_df\n",
    "        gc.collect()\n",
    "\n",
    "    print(f\"\\nSmall diff processing complete. ✅\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Input file not found at '{INPUT_CSV}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd97103",
   "metadata": {},
   "source": [
    "# new tokenizing + embedding\n",
    "previous methot returns somany null valuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27e550d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import gc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3ae2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "CLEAN_SOURCE_CSV = \"../data/final_dataset_with_full_diffs_CLEANED.csv\"\n",
    "FINAL_OUTPUT_DIR = \"../data/final_embeddings_sliding_window\"\n",
    "ERROR_LOG_PATH = \"../logs/embedding_errors.log\"\n",
    "CHUNK_SIZE = 10 # How many rows from the CSV to process at a time\n",
    "MODEL_NAME = \"microsoft/codebert-base\"\n",
    "\n",
    "# --- Sliding Window Configuration ---\n",
    "MAX_LENGTH = 512  # The model's max token length\n",
    "OVERLAP = 50      # How many tokens to overlap between chunks\n",
    "STRIDE = MAX_LENGTH - OVERLAP # The step size for the window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb51e45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Resuming: Found 0 processed embedding batches. Skipping first 0 rows.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b30bcb7d7f645248d4c24eb8f09dc14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing CSV Chunks: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (145865 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Embedding Generation Complete --- ✅\n"
     ]
    }
   ],
   "source": [
    "# --- Setup ---\n",
    "os.makedirs(FINAL_OUTPUT_DIR, exist_ok=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
    "model.eval()\n",
    "\n",
    "# --- Checkpointing ---\n",
    "processed_chunks = len(os.listdir(FINAL_OUTPUT_DIR))\n",
    "rows_to_skip = processed_chunks * CHUNK_SIZE\n",
    "print(f\"Resuming: Found {processed_chunks} processed embedding batches. Skipping first {rows_to_skip} rows.\")\n",
    "\n",
    "# --- Main Resumable Loop ---\n",
    "try:\n",
    "    # csv_reader = pd.read_csv(CLEAN_SOURCE_CSV, chunksize=CHUNK_SIZE, skiprows=range(1, rows_to_skip + 1))\n",
    "    csv_reader = pd.read_csv(CLEAN_SOURCE_CSV, chunksize=CHUNK_SIZE, skiprows=range(1, rows_to_skip + 1),nrows=10)\n",
    "    \n",
    "    for i, chunk_df in enumerate(tqdm(csv_reader, desc=\"Processing CSV Chunks\")):\n",
    "        current_chunk_index = processed_chunks + i\n",
    "        \n",
    "        batch_embeddings = []\n",
    "        batch_labels = []\n",
    "        \n",
    "        # Iterate through each row in the current pandas chunk\n",
    "        for _, row in chunk_df.iterrows():\n",
    "            try:\n",
    "                diff_text = str(row['diff'])\n",
    "                label = int(row['is_bug_introducing'])\n",
    "                \n",
    "                # Tokenize the entire diff without truncation first\n",
    "                all_input_ids = tokenizer.encode(diff_text, add_special_tokens=False)\n",
    "                \n",
    "                final_embedding = None\n",
    "                \n",
    "                if len(all_input_ids) <= MAX_LENGTH - 2: # -2 for [CLS] and [SEP]\n",
    "                    # If the diff is short, process it normally\n",
    "                    inputs = tokenizer(diff_text, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\").to(device)\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(**inputs)\n",
    "                    final_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "                else:\n",
    "                    # --- Sliding Window Logic ---\n",
    "                    chunk_embeddings = []\n",
    "                    # Create overlapping chunks of token IDs\n",
    "                    for start in range(0, len(all_input_ids), STRIDE):\n",
    "                        end = start + MAX_LENGTH - 2\n",
    "                        chunk_ids = all_input_ids[start:end]\n",
    "                        \n",
    "                        # Add special tokens and convert to tensor\n",
    "                        input_tensor = torch.tensor([tokenizer.cls_token_id] + chunk_ids + [tokenizer.sep_token_id]).unsqueeze(0).to(device)\n",
    "                        \n",
    "                        # Get embedding for this chunk\n",
    "                        with torch.no_grad():\n",
    "                            outputs = model(input_tensor)\n",
    "                        chunk_embeddings.append(outputs.last_hidden_state[:, 0, :])\n",
    "                    \n",
    "                    # --- Pooling Step ---\n",
    "                    # Average the embeddings of all chunks to get a single vector\n",
    "                    if chunk_embeddings:\n",
    "                        stacked_embeddings = torch.stack(chunk_embeddings)\n",
    "                        final_embedding = torch.mean(stacked_embeddings, dim=0).cpu().numpy()\n",
    "\n",
    "                if final_embedding is not None:\n",
    "                    batch_embeddings.append(final_embedding.flatten())\n",
    "                    batch_labels.append(label)\n",
    "\n",
    "            except Exception as e:\n",
    "                with open(ERROR_LOG_PATH, 'a', encoding='utf-8') as f:\n",
    "                    f.write(f\"Error processing commit {row.get('commit_hash', 'N/A')}: {e}\\n\")\n",
    "\n",
    "        # Save the collected embeddings and labels for this chunk\n",
    "        if batch_embeddings:\n",
    "            output_path = os.path.join(FINAL_OUTPUT_DIR, f\"batch_{current_chunk_index}.npz\")\n",
    "            np.savez_compressed(output_path, embeddings=np.array(batch_embeddings), labels=np.array(batch_labels))\n",
    "\n",
    "        # Clean up memory\n",
    "        del batch_embeddings, batch_labels, chunk_df\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"\\n--- Embedding Generation Complete --- ✅\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Clean file not found at '{CLEAN_SOURCE_CSV}'. Please run the cleaning step first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc0c017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 .npz files to process.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d65a10c156404ab850a2e5a4f09983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting .npz to DataFrame chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating all chunks into the final DataFrame...\n",
      "\n",
      "Successfully converted and saved data to 'data/final_embeddings_sliding_window.csv' ✅\n",
      "Final DataFrame shape: (10, 769)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "EMBEDDINGS_DIR = FINAL_OUTPUT_DIR # Use the directory from the previous cell\n",
    "OUTPUT_CSV_PATH = \"../data/final_embeddings_sliding_window.csv\"\n",
    "\n",
    "# --- Logic ---\n",
    "# Get a list of all the .npz files generated in the previous step\n",
    "try:\n",
    "    npz_files = [f for f in os.listdir(EMBEDDINGS_DIR) if f.endswith('.npz')]\n",
    "    if not npz_files:\n",
    "        print(f\"No .npz files found in '{EMBEDDINGS_DIR}'. Nothing to convert.\")\n",
    "    else:\n",
    "        print(f\"Found {len(npz_files)} .npz files to process.\")\n",
    "        \n",
    "        list_of_dfs = []\n",
    "\n",
    "        # Loop through each file, load it, and convert to a DataFrame\n",
    "        for file_name in tqdm(npz_files, desc=\"Converting .npz to DataFrame chunks\"):\n",
    "            file_path = os.path.join(EMBEDDINGS_DIR, file_name)\n",
    "            \n",
    "            with np.load(file_path) as data:\n",
    "                embeddings = data['embeddings']\n",
    "                labels = data['labels']\n",
    "                \n",
    "                # Create a DataFrame for the embeddings\n",
    "                embedding_df = pd.DataFrame(embeddings, columns=[f'embedding_{i}' for i in range(embeddings.shape[1])])\n",
    "                \n",
    "                # Create a DataFrame for the labels\n",
    "                labels_df = pd.DataFrame(labels, columns=['label'])\n",
    "                \n",
    "                # Combine them side-by-side\n",
    "                chunk_df = pd.concat([labels_df, embedding_df], axis=1)\n",
    "                list_of_dfs.append(chunk_df)\n",
    "\n",
    "        # Concatenate all the small DataFrames into one large DataFrame\n",
    "        print(\"Concatenating all chunks into the final DataFrame...\")\n",
    "        final_df = pd.concat(list_of_dfs, ignore_index=True)\n",
    "\n",
    "        # Save the final DataFrame to a CSV file\n",
    "        final_df.to_csv(OUTPUT_CSV_PATH, index=False)\n",
    "        \n",
    "        print(f\"\\nSuccessfully converted and saved data to '{OUTPUT_CSV_PATH}' ✅\")\n",
    "        print(f\"Final DataFrame shape: {final_df.shape}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: The directory '{EMBEDDINGS_DIR}' was not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "288d000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import gc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e28c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "CLEAN_SOURCE_CSV = \"../data/final_dataset_with_full_diffs_CLEANED.csv\"\n",
    "FINAL_OUTPUT_CSV = \"../data/final_embeddings_sliding_window/final_dataset_with_embeddings.csv\" # The single, final output file\n",
    "ERROR_LOG_PATH = \"../logs/embedding_errors.log\"\n",
    "# CHUNK_SIZE = 64\n",
    "CHUNK_SIZE = 32\n",
    "MODEL_NAME = \"microsoft/codebert-base\"\n",
    "\n",
    "# --- Sliding Window Configuration ---\n",
    "MAX_LENGTH = 512\n",
    "OVERLAP = 50\n",
    "STRIDE = MAX_LENGTH - OVERLAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45b2aab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# --- Setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
    "model.eval()\n",
    "\n",
    "# --- Checkpointing: Determine where to resume from by counting rows in the output CSV ---\n",
    "rows_to_skip = 0\n",
    "if os.path.exists(FINAL_OUTPUT_CSV):\n",
    "    with open(FINAL_OUTPUT_CSV, 'r', encoding='utf-8') as f:\n",
    "        # -1 to not count the header row\n",
    "        rows_to_skip = max(0, sum(1 for line in f) - 1)\n",
    "    print(f\"Resuming: Found {rows_to_skip} rows already processed in '{FINAL_OUTPUT_CSV}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec2f3615",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_skip = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b488165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32844517664e40e8b1dd1097321e4c01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Embeddings: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Main Resumable Loop ---\n",
    "try:\n",
    "    # csv_reader = pd.read_csv(CLEAN_SOURCE_CSV, chunksize=CHUNK_SIZE, skiprows=range(1, rows_to_skip + 1))\n",
    "    csv_reader = pd.read_csv(CLEAN_SOURCE_CSV, chunksize=CHUNK_SIZE, skiprows=range(1, rows_to_skip + 1),nrows=100)\n",
    "    \n",
    "    # We write the header only if the file is new or empty\n",
    "    write_header = (rows_to_skip == 0)\n",
    "\n",
    "    for chunk_df in tqdm(csv_reader, desc=\"Generating Embeddings\"):\n",
    "        batch_embeddings = []\n",
    "        \n",
    "        for _, row in chunk_df.iterrows():\n",
    "            try:\n",
    "                diff_text = str(row['diff'])\n",
    "                all_input_ids = tokenizer.encode(diff_text, add_special_tokens=False)\n",
    "                final_embedding_vector = None\n",
    "                \n",
    "                if len(all_input_ids) <= MAX_LENGTH - 2:\n",
    "                    inputs = tokenizer(diff_text, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\").to(device)\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(**inputs)\n",
    "                    final_embedding_vector = outputs.last_hidden_state[:, 0, :].cpu().numpy().flatten()\n",
    "                else:\n",
    "                    # Sliding Window Logic\n",
    "                    chunk_embeddings = []\n",
    "                    for start in range(0, len(all_input_ids), STRIDE):\n",
    "                        chunk_ids = all_input_ids[start : start + MAX_LENGTH - 2]\n",
    "                        input_tensor = torch.tensor([tokenizer.cls_token_id] + chunk_ids + [tokenizer.sep_token_id]).unsqueeze(0).to(device)\n",
    "                        with torch.no_grad():\n",
    "                            outputs = model(input_tensor)\n",
    "                        chunk_embeddings.append(outputs.last_hidden_state[:, 0, :])\n",
    "                    \n",
    "                    if chunk_embeddings:\n",
    "                        stacked_embeddings = torch.stack(chunk_embeddings)\n",
    "                        final_embedding_vector = torch.mean(stacked_embeddings, dim=0).cpu().numpy().flatten()\n",
    "\n",
    "                if final_embedding_vector is not None:\n",
    "                    batch_embeddings.append(final_embedding_vector)\n",
    "                else:\n",
    "                    # Append a zero vector if embedding fails for any reason\n",
    "                    batch_embeddings.append(np.zeros(model.config.hidden_size))\n",
    "\n",
    "            except Exception as e:\n",
    "                batch_embeddings.append(np.zeros(model.config.hidden_size)) # Append zero vector on error\n",
    "                with open(ERROR_LOG_PATH, 'a', encoding='utf-8') as f:\n",
    "                    f.write(f\"Error on commit {row.get('commit_hash', 'N/A')}: {e}\\n\")\n",
    "\n",
    "        # --- Combine original data with new embeddings ---\n",
    "        embedding_df = pd.DataFrame(batch_embeddings, columns=[f'emb_{j}' for j in range(model.config.hidden_size)])\n",
    "        \n",
    "        # Reset indices to ensure correct side-by-side concatenation\n",
    "        chunk_df.reset_index(drop=True, inplace=True)\n",
    "        chunk_df.drop(columns=['diff'], inplace=True)\n",
    "        embedding_df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        combined_chunk_df = pd.concat([chunk_df, embedding_df], axis=1)\n",
    "        \n",
    "        # --- Append the combined chunk to the final CSV ---\n",
    "        combined_chunk_df.to_csv(FINAL_OUTPUT_CSV, mode='a', header=write_header, index=False)\n",
    "        write_header = False # Ensure header is only written once\n",
    "\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"\\n--- Final Dataset Generation Complete --- ✅\")\n",
    "    print(f\"Dataset with all original metadata and embeddings saved to '{FINAL_OUTPUT_CSV}'.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Clean file not found at '{CLEAN_SOURCE_CSV}'. Please run the cleaning step first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01d089b5",
   "metadata": {},
   "source": [
    "# Data preparation for codeBERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33375dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import git\n",
    "from loguru import logger\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Logger config\n",
    "logger.add(\"logs/diff_extraction.log\")\n",
    "\n",
    "logger.info(\"Libraries imported and logger configured for Feature Extraction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1058842a",
   "metadata": {},
   "source": [
    "Load the last data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9c1b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_DATASET_PATH = \"../data/final/final_labeled_training_dataset.csv\"\n",
    "df = pd.read_csv(FINAL_DATASET_PATH)\n",
    "df = df[['commit_hash', 'is_bug_introducing']].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ab1344",
   "metadata": {},
   "source": [
    "Function to get commit diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d41c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_PATH = \"../../ballerina-lang/\" # Make sure this path is correct\n",
    "repo = git.Repo(REPO_PATH)\n",
    "\n",
    "def get_commit_diff(commit_hash):\n",
    "    try:\n",
    "        commit = repo.commit(commit_hash)\n",
    "        parent = commit.parents[0] if commit.parents else None\n",
    "        if parent:\n",
    "            return repo.git.diff(parent, commit, '--no-color', '--unified=0')\n",
    "        return \"\" # No parent, no diff\n",
    "        \n",
    "    except Exception:\n",
    "        logger.error(f\"Error retrieving diff for commit {commit_hash}\")\n",
    "        return \"\" # Handle errors or missing commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bacf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previous progress\n",
    "df = pd.read_pickle('temp_backup.pkl')  # or 'commits_with_diffs.pkl'\n",
    "print(f\"Loaded {len(df)} previously processed commits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23002988",
   "metadata": {},
   "source": [
    "Create lists of diffs and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27630ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ultra simple - just replace your failing code with this:\n",
    "\n",
    "print(\"Processing commits in small batches to avoid memory issues...\")\n",
    "\n",
    "# Process in small chunks and save frequently\n",
    "chunk_size = 30  # Very small to be safe\n",
    "all_results = []\n",
    "\n",
    "for i in tqdm(range(0, len(df), chunk_size), desc=\"Processing\"):\n",
    "    chunk = df.iloc[i:i+chunk_size]\n",
    "    \n",
    "    for _, row in chunk.iterrows():\n",
    "        try:\n",
    "            diff = get_commit_diff(row['commit_hash'])\n",
    "            if diff.strip():  # Only keep non-empty diffs\n",
    "                new_row = row.copy()\n",
    "                new_row['diff'] = diff\n",
    "                all_results.append(new_row)\n",
    "        except:\n",
    "            continue  # Skip problematic commits\n",
    "    \n",
    "    # Save progress every 100 commits\n",
    "    if len(all_results) % 100 == 0 and all_results:\n",
    "        pd.DataFrame(all_results).to_pickle('temp_backup.pkl')\n",
    "        print(f\"üíæ Backup saved - {len(all_results)} commits processed\")\n",
    "\n",
    "# Final result\n",
    "if all_results:\n",
    "    df = pd.DataFrame(all_results).reset_index(drop=True)\n",
    "    df.to_pickle('commits_with_diffs.pkl')  # Save final version\n",
    "    print(f\"‚úÖ Success! {len(df)} commits with diffs\")\n",
    "else:\n",
    "    print(\"‚ùå No commits with diffs found\")\n",
    "    df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f82adf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Note: This can be slow. You might want to run it once and save the results.\n",
    "tqdm.pandas()  # Enables progress_apply\n",
    "print(\"Extracting diffs for each commit...\")\n",
    "\n",
    "df['diff'] = df['commit_hash'].progress_apply(get_commit_diff)\n",
    "df = df[df['diff'] != \"\"] # Filter out commits with no diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952a901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/final/final_labeled_training_dataset_with_diffs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa82f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use a chronological split (assuming df is still sorted by date)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, shuffle=False)\n",
    "\n",
    "train_texts, train_labels = train_df['diff'].tolist(), train_df['is_bug_introducing'].tolist()\n",
    "test_texts, test_labels = test_df['diff'].tolist(), test_df['is_bug_introducing'].tolist()\n",
    "\n",
    "# --- 4. Tokenize the data ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# --- 5. Create a PyTorch Dataset object ---\n",
    "class BugDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = BugDataset(train_encodings, train_labels)\n",
    "test_dataset = BugDataset(test_encodings, test_labels)\n",
    "\n",
    "print(\"Data prepared and tokenized for CodeBERT.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

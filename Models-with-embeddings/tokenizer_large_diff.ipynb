{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d978c51",
   "metadata": {},
   "source": [
    "# tokenizing large diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90dd481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import gc\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf86061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "LARGE_DIFFS_CSV = \"data/large_diffs.csv\"\n",
    "SMALL_DIFFS_CSV = \"data/small_diffs.csv\"\n",
    "TOKENIZED_DATA_DIR = \"data/tokenized_data_test4/large_diffs\"\n",
    "LABEL_ERROR_LOG_PATH = \"logs/label_parsing_errors.log\" # New log file for bad labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b700ca34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV field size limit increased to handle large fields.\n",
      "Header read successfully.\n",
      "Processing large diffs: Resuming, found 0 already processed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f24f4ae9eb449a99b221f53a4453873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Large Diffs:   0%|          | 0/479 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Large diff processing complete. ✅\n",
      "Any label parsing errors have been logged to 'logs/label_parsing_errors.log'.\n"
     ]
    }
   ],
   "source": [
    "# --- Setup ---\n",
    "os.makedirs(TOKENIZED_DATA_DIR, exist_ok=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "# --- FIX 1: Increase the CSV field size limit to prevent crashes ---\n",
    "max_int = sys.maxsize\n",
    "while True:\n",
    "    try:\n",
    "        csv.field_size_limit(max_int)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        max_int = int(max_int / 10)\n",
    "print(f\"CSV field size limit increased to handle large fields.\")\n",
    "\n",
    "# --- Get header from the small_diffs file ---\n",
    "try:\n",
    "    with open(SMALL_DIFFS_CSV, 'r', encoding='utf-8') as f:\n",
    "        header = next(csv.reader(f))\n",
    "    print(\"Header read successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: '{SMALL_DIFFS_CSV}' not found.\")\n",
    "    raise\n",
    "\n",
    "# --- Checkpointing ---\n",
    "processed_files_count = len(os.listdir(TOKENIZED_DATA_DIR))\n",
    "print(f\"Processing large diffs: Resuming, found {processed_files_count} already processed.\")\n",
    "\n",
    "# --- Main Memory-Safe Loop with Enhanced Logging ---\n",
    "with open(LARGE_DIFFS_CSV, 'r', encoding='utf-8') as infile, \\\n",
    "     open(LABEL_ERROR_LOG_PATH, 'a', encoding='utf-8') as error_log:\n",
    "    \n",
    "    reader = csv.reader(infile)\n",
    "    try:\n",
    "        total_rows = sum(1 for row in reader)\n",
    "        infile.seek(0)\n",
    "    except Exception:\n",
    "        total_rows = None\n",
    "\n",
    "    # Skip already processed rows\n",
    "    for _ in range(processed_files_count):\n",
    "        next(reader, None)\n",
    "\n",
    "    for i, row in enumerate(tqdm(reader, desc=\"Processing Large Diffs\", initial=processed_files_count, total=total_rows)):\n",
    "        file_index = processed_files_count + i\n",
    "        \n",
    "        row_dict = dict(zip(header, row))\n",
    "        \n",
    "        diff_text = row_dict.get('diff', '')\n",
    "        commit_hash = row_dict.get('commit_hash', 'UNKNOWN_HASH')\n",
    "        \n",
    "        # --- FIX 2: Enhanced Label Parsing and Logging ---\n",
    "        label_str = row_dict.get('is_bug_introducing', '0')\n",
    "        try:\n",
    "            label = int(float(label_str))\n",
    "        except (ValueError, TypeError):\n",
    "            # If parsing fails, log the commit hash and bad label to a file\n",
    "            error_log.write(f\"Commit: {commit_hash}, Unparseable_Label: '{label_str}'\\n\")\n",
    "            label = 0 # Default to 0 and continue\n",
    "            \n",
    "        # Tokenize just this one diff\n",
    "        encoding = tokenizer(str(diff_text), truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "        # Save the single tokenized item to its own file\n",
    "        chunk_path = os.path.join(TOKENIZED_DATA_DIR, f\"large_item_{file_index}.pt\")\n",
    "        torch.save({\n",
    "            'input_ids': encoding['input_ids'],\n",
    "            'attention_mask': encoding['attention_mask'],\n",
    "            'labels': torch.tensor([label])\n",
    "        }, chunk_path)\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "print(f\"\\nLarge diff processing complete. ✅\")\n",
    "print(f\"Any label parsing errors have been logged to '{LABEL_ERROR_LOG_PATH}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

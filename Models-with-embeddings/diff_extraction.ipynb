{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6054905b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import git\n",
    "import os\n",
    "import numpy as np\n",
    "import gc\n",
    "from pymongo import MongoClient\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae33239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to MongoDB...\n",
      "Found 0 commits already in the database.\n",
      "New commits to process: 125825.\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "FINAL_DATASET_PATH = \"../data/final/final_labeled_training_dataset.csv\"\n",
    "REPO_PATH = \"../../ballerina-lang/\"\n",
    "BATCH_SIZE = 500\n",
    "MAX_WORKERS = 10\n",
    "MONGO_URI = \"mongodb://localhost:27017/\"\n",
    "MONGO_DB_NAME = \"bug_prediction_project_2\"\n",
    "MONGO_COLLECTION_NAME = \"commits_with_diffs\"\n",
    "ERROR_LOG_PATH = \"diff_errors.log\"  # New file to store errors\n",
    "\n",
    "# --- Environment variable to prevent Git hangs ---\n",
    "os.environ[\"GIT_TERMINAL_PROMPT\"] = \"0\"\n",
    "\n",
    "\n",
    "# --- NEW Worker Function that returns errors ---\n",
    "def get_commit_diff_worker(commit_hash, repo_path):\n",
    "    \"\"\"\n",
    "    Robust worker that returns a (diff, error) tuple.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        repo = git.Repo(repo_path)\n",
    "        commit = repo.commit(commit_hash)\n",
    "        if commit.parents:\n",
    "            parent = commit.parents[0]\n",
    "            diff_text = repo.git.diff(parent, commit, \"--no-color\", \"--unified=0\")\n",
    "            return (diff_text, None)  # Success\n",
    "        return (\"\", None)  # Success (no parent)\n",
    "    except Exception as e:\n",
    "        return (None, str(e))  # Failure, return the error message\n",
    "\n",
    "\n",
    "# --- Connect to MongoDB and Checkpoint (Same as before) ---\n",
    "print(f\"Connecting to MongoDB...\")\n",
    "client = MongoClient(MONGO_URI)\n",
    "db = client[MONGO_DB_NAME]\n",
    "collection = db[MONGO_COLLECTION_NAME]\n",
    "collection.create_index(\"commit_hash\", unique=True)\n",
    "\n",
    "processed_hashes = {\n",
    "    doc[\"commit_hash\"] for doc in collection.find({}, {\"commit_hash\": 1, \"_id\": 0})\n",
    "}\n",
    "print(f\"Found {len(processed_hashes)} commits already in the database.\")\n",
    "\n",
    "df = pd.read_csv(FINAL_DATASET_PATH)\n",
    "df_to_process = df[~df[\"commit_hash\"].isin(processed_hashes)].copy()\n",
    "print(f\"New commits to process: {len(df_to_process)}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4946c7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pradishan\\code\\wso2-AI-Tool\\.venv\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c918c1b69e334631ba63547c1176f482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Overall Progress:   0%|          | 0/251 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "633b817d61db4cf2946320b94196a201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 1/251:   0%|          | 0/502 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 1: Inserted 502 documents into MongoDB.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eca2867564148baaf1996488397bb02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 2/251:   0%|          | 0/502 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 2: Error during bulk insert. Error: BSONObj size: 23535236 (0x1671E84) is invalid. Size must be between 0 and 16793600(16MB) First element: _id: ObjectId('687fa49789c8eb871baf732b'), full error: {'ok': 0.0, 'errmsg': \"BSONObj size: 23535236 (0x1671E84) is invalid. Size must be between 0 and 16793600(16MB) First element: _id: ObjectId('687fa49789c8eb871baf732b')\", 'code': 10334, 'codeName': 'BSONObjectTooLarge'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa3b6aa2c15f4a0f8cba079a114346bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 3/251:   0%|          | 0/502 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 3: Inserted 502 documents into MongoDB.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab39a3f18ab845e49c2d6c2a0c2136e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 4/251:   0%|          | 0/502 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 4: Inserted 502 documents into MongoDB.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5501b5b0c464867b8df6f0481dc725d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 5/251:   0%|          | 0/502 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 5: Inserted 502 documents into MongoDB.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5714b95018ab4dd1b601b7eb7b6fa769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 6/251:   0%|          | 0/502 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 6: Inserted 502 documents into MongoDB.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f09f30cb58d940ea8f1b33e146292f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 7/251:   0%|          | 0/502 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 7: Error during bulk insert. Error: BSON document too large (190630005 bytes) - the connected server supports BSON document sizes up to 16777216 bytes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed7806d1c6d8481f82178b8d44fb8dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 8/251:   0%|          | 0/502 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 8: Inserted 502 documents into MongoDB.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef9e330ec87b48bebe06beb07e4cdabd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 9/251:   0%|          | 0/502 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 9: Inserted 502 documents into MongoDB.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e43a597728d436eb5f6bb7709aac8f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 10/251:   0%|          | 0/502 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 10: Error during bulk insert. Error: BSONObj size: 34280911 (0x20B15CF) is invalid. Size must be between 0 and 16793600(16MB) First element: _id: ObjectId('687fa8fa89c8eb871baf82da'), full error: {'ok': 0.0, 'errmsg': \"BSONObj size: 34280911 (0x20B15CF) is invalid. Size must be between 0 and 16793600(16MB) First element: _id: ObjectId('687fa8fa89c8eb871baf82da')\", 'code': 10334, 'codeName': 'BSONObjectTooLarge'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac1bf30f8fee47d9bc439fba60d4f2be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 11/251:   0%|          | 0/502 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 11: Error during bulk insert. Error: BSONObj size: 17305913 (0x1081139) is invalid. Size must be between 0 and 16793600(16MB) First element: _id: ObjectId('687fa97e89c8eb871baf842f'), full error: {'ok': 0.0, 'errmsg': \"BSONObj size: 17305913 (0x1081139) is invalid. Size must be between 0 and 16793600(16MB) First element: _id: ObjectId('687fa97e89c8eb871baf842f')\", 'code': 10334, 'codeName': 'BSONObjectTooLarge'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78cf81234f004af2a73b36590e53c4a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 12/251:   0%|          | 0/502 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 12: Error during bulk insert. Error: BSONObj size: 28338618 (0x1B069BA) is invalid. Size must be between 0 and 16793600(16MB) First element: _id: ObjectId('687faa0b89c8eb871baf871f'), full error: {'ok': 0.0, 'errmsg': \"BSONObj size: 28338618 (0x1B069BA) is invalid. Size must be between 0 and 16793600(16MB) First element: _id: ObjectId('687faa0b89c8eb871baf871f')\", 'code': 10334, 'codeName': 'BSONObjectTooLarge'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "716b20dc763747f6a3358f5e6e95b331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 13/251:   0%|          | 0/502 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 13: Error during bulk insert. Error: BSONObj size: 18824365 (0x11F3CAD) is invalid. Size must be between 0 and 16793600(16MB) First element: _id: ObjectId('687faaa189c8eb871baf880b'), full error: {'ok': 0.0, 'errmsg': \"BSONObj size: 18824365 (0x11F3CAD) is invalid. Size must be between 0 and 16793600(16MB) First element: _id: ObjectId('687faaa189c8eb871baf880b')\", 'code': 10334, 'codeName': 'BSONObjectTooLarge'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b5d8c0f1d34f5e82b0955d180712c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 14/251:   0%|          | 0/502 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Re-associate results and handle errors\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(ERROR_LOG_PATH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m error_file:\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m record, (diff, error) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch_records, results_iterator):\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m error:\n\u001b[0;32m     21\u001b[0m             \u001b[38;5;66;03m# If an error occurred, write it to the log file\u001b[39;00m\n\u001b[0;32m     22\u001b[0m             error_file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCommit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecord[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcommit_hash\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pradishan\\code\\wso2-AI-Tool\\.venv\\lib\\site-packages\\tqdm\\notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[1;32m--> 250\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m    251\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pradishan\\code\\wso2-AI-Tool\\.venv\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\concurrent\\futures\\_base.py:600\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[1;34m()\u001b[0m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[0;32m    598\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[0;32m    599\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 600\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    601\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    602\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mresult(end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\concurrent\\futures\\_base.py:435\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m--> 435\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- Main Loop with Error Logging ---\n",
    "df_batches = np.array_split(df_to_process, max(1, len(df_to_process) // BATCH_SIZE))\n",
    "worker_with_path = partial(get_commit_diff_worker, repo_path=REPO_PATH)\n",
    "\n",
    "for i, batch_df in enumerate(tqdm(df_batches, desc=\"Overall Progress\")):\n",
    "    batch_records = batch_df.to_dict(\"records\")\n",
    "    commit_hashes_in_batch = [rec[\"commit_hash\"] for rec in batch_records]\n",
    "\n",
    "    processed_records = []\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        results_iterator = tqdm(\n",
    "            executor.map(worker_with_path, commit_hashes_in_batch),\n",
    "            total=len(commit_hashes_in_batch),\n",
    "            desc=f\"Batch {i + 1}/{len(df_batches)}\",\n",
    "        )\n",
    "\n",
    "        # Re-associate results and handle errors\n",
    "        with open(ERROR_LOG_PATH, \"a\", encoding=\"utf-8\") as error_file:\n",
    "            for record, (diff, error) in zip(batch_records, results_iterator):\n",
    "                if error:\n",
    "                    # If an error occurred, write it to the log file\n",
    "                    error_file.write(\n",
    "                        f\"Commit: {record['commit_hash']}\\nError: {error}\\n---\\n\"\n",
    "                    )\n",
    "\n",
    "                # The 'diff' field will be empty for both errors and commits with no diff\n",
    "                record[\"diff\"] = diff if diff is not None else \"\"\n",
    "                processed_records.append(record)\n",
    "\n",
    "    if processed_records:\n",
    "        try:\n",
    "            collection.insert_many(processed_records, ordered=False)\n",
    "            print(\n",
    "                f\"\\nBatch {i + 1}: Inserted {len(processed_records)} documents into MongoDB.\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"\\nBatch {i + 1}: Error during bulk insert. Error: {e}\")\n",
    "\n",
    "    # Memory cleanup\n",
    "    del processed_records\n",
    "    del batch_records\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\n--- Process Complete ---\")\n",
    "print(f\"Any errors encountered have been saved to '{ERROR_LOG_PATH}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "716ce6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "# Read the CSV file in chunks\n",
    "chunk_size = 100000  # Adjust this value based on your system's memory\n",
    "chunks = []\n",
    "\n",
    "try:\n",
    "    # Create a chunk iterator\n",
    "    chunk_iterator = pd.read_csv(\"final_dataset_with_diffs.csv\", chunksize=chunk_size)\n",
    "\n",
    "    # Process each chunk\n",
    "    for chunk in chunk_iterator:\n",
    "        # Process the chunk as needed\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    # Combine all chunks\n",
    "    df2 = pd.concat(chunks, ignore_index=True)\n",
    "    print(f\"Successfully loaded {len(df2)} rows\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n",
    "\n",
    "# Free up memory\n",
    "del chunks\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a088c9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6951.470607757568 MB\n"
     ]
    }
   ],
   "source": [
    "sample = pd.read_csv(\"final_dataset_with_diffs.csv\",nrows=10000)\n",
    "print(sample.memory_usage(deep=True).sum() / (1024**2), \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef878079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_test = pd.read_csv(\"final_dataset_with_diffs.csv\", chunksize=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e060c08",
   "metadata": {},
   "source": [
    "# XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c97bc8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "import xgboost as xgb\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63c62c8",
   "metadata": {},
   "source": [
    "configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57caded2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a grid with 48 hyperparameter combinations to test.\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100, 200, 300],\n",
    "    \"max_depth\": [None, 10, 20],\n",
    "    \"learning_rate\": [0.05, 0.1],\n",
    "    \"subsample\": [0.7, 1.0],\n",
    "}\n",
    "\n",
    "# Create a list of all possible combinations\n",
    "grid = list(ParameterGrid(param_grid))\n",
    "\n",
    "print(f\"Created a grid with {len(grid)} hyperparameter combinations to test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7616b3ba",
   "metadata": {},
   "source": [
    "Load and Split Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f7a004",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_DATASET_PATH = \"data/final_embedding_dataset.csv\"\n",
    "df = pd.read_csv(FINAL_DATASET_PATH)\n",
    "df = df.dropna(subset=[\"commit_hash\", \"author_email\", \"commit_date\", \"is_bug_introducing\", \"diff\"])\n",
    "df[\"commit_date\"] = pd.to_datetime(df[\"commit_date\"],utc=True)\n",
    "df.sort_values(by=\"commit_date\", inplace=True)\n",
    "\n",
    "X = df.drop(\n",
    "    columns=[\"commit_hash\", \"author_email\", \"commit_date\", \"is_bug_introducing\", \"diff\"]\n",
    ")\n",
    "y = df[\"is_bug_introducing\"]\n",
    "\n",
    "split_point = int(len(df) * 0.80)\n",
    "X_train, X_test = X.iloc[:split_point], X.iloc[split_point:]\n",
    "y_train, y_test = y.iloc[:split_point], y.iloc[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465b4a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b086825",
   "metadata": {},
   "source": [
    "## functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b243ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_xgb(params, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Trains an XGBoost model and returns the model and its performance metrics.\"\"\"\n",
    "    # Use 'use_label_encoder=False' and 'eval_metric' to avoid common warnings\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        random_state=42, eval_metric=\"logloss\", n_jobs=-1, **params\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "    y_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_test, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_test, y_pred, zero_division=0),\n",
    "        \"roc_auc\": roc_auc_score(y_test, y_pred_proba),\n",
    "        \"training_duration\": end_time - start_time,\n",
    "    }\n",
    "    return xgb_model, metrics\n",
    "\n",
    "\n",
    "def log_to_mlflow(run_name, params, metrics, model, feature_names):\n",
    "    \"\"\"Logs all experiment data for a single run to MLflow.\"\"\"\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        # Log hyperparameters\n",
    "        for key, value in params.items():\n",
    "            mlflow.log_param(key, value)\n",
    "\n",
    "        # Log metrics\n",
    "        for key, value in metrics.items():\n",
    "            mlflow.log_metric(key, value)\n",
    "\n",
    "        # Log the XGBoost model\n",
    "        mlflow.xgboost.log_model(model, \"model\")\n",
    "\n",
    "        # Create and log feature importance plot\n",
    "        fig = plot_feature_importance(model, feature_names)\n",
    "        mlflow.log_figure(fig, \"feature_importance.png\")\n",
    "        plt.close(fig)  # Prevent inline display\n",
    "\n",
    "        # Create feature importance DataFrame and save as CSV\n",
    "        feature_importance_df = pd.DataFrame(\n",
    "            {\"feature\": feature_names, \"importance\": model.feature_importances_}\n",
    "        ).sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "        # Save and log feature importance as CSV\n",
    "        importance_path = \"feature_importance.csv\"\n",
    "        feature_importance_df.to_csv(importance_path, index=False)\n",
    "        mlflow.log_artifact(importance_path)\n",
    "        os.remove(importance_path)  # Clean up temporary file\n",
    "\n",
    "        # Log model parameters as tags for easy filtering\n",
    "        mlflow.set_tag(\"model_type\", \"XGBoost\")\n",
    "        mlflow.set_tag(\"data_type\", \"embeddings\")\n",
    "\n",
    "\n",
    "print(\"Helper functions defined successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7390fde9",
   "metadata": {},
   "source": [
    "## Model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f70d344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLflow setup\n",
    "mlflow.set_tracking_uri(\"file:./mlruns\")\n",
    "\n",
    "# Set the experiment name\n",
    "experiment_name = \"XGBoost_Experiment_with_Embeddings\"\n",
    "\n",
    "# Set or create experiment\n",
    "try:\n",
    "    experiment_id = mlflow.create_experiment(experiment_name)\n",
    "except mlflow.exceptions.MlflowException:\n",
    "    experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "\n",
    "mlflow.set_experiment(experiment_name)\n",
    "print(f\"MLflow experiment set: {experiment_name}\")\n",
    "print(f\"Tracking URI: {mlflow.get_tracking_uri()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cefc9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Starting XGBoost Hyperparameter Search ---\")\n",
    "\n",
    "results_list_xgb = []\n",
    "best_f1_score = 0\n",
    "best_model = None\n",
    "best_params = None\n",
    "best_run_id = None\n",
    "\n",
    "for i, params in enumerate(tqdm(grid, desc=\"Training XGBoost Models\")):\n",
    "    run_name = f\"xgb_run_{i:03d}\"\n",
    "\n",
    "    # 1. Train and evaluate the XGBoost model\n",
    "    model, metrics = train_and_evaluate_xgb(params, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # 2. Log everything to MLflow for this run\n",
    "    with mlflow.start_run(run_name=run_name) as run:\n",
    "        # Log hyperparameters\n",
    "        for key, value in params.items():\n",
    "            mlflow.log_param(key, value)\n",
    "\n",
    "        # Log metrics\n",
    "        for key, value in metrics.items():\n",
    "            mlflow.log_metric(key, value)\n",
    "\n",
    "        # Log the XGBoost model\n",
    "        mlflow.xgboost.log_model(model, \"model\")\n",
    "\n",
    "        # Create and log feature importance plot\n",
    "        fig = plot_feature_importance(model, X_train.columns)\n",
    "        mlflow.log_figure(fig, \"feature_importance.png\")\n",
    "        plt.close(fig)  # Prevent inline display\n",
    "\n",
    "        # Create feature importance DataFrame and save as CSV\n",
    "        feature_importance_df = pd.DataFrame(\n",
    "            {\"feature\": X_train.columns, \"importance\": model.feature_importances_}\n",
    "        ).sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "        # Save and log feature importance as CSV\n",
    "        importance_path = \"feature_importance.csv\"\n",
    "        feature_importance_df.to_csv(importance_path, index=False)\n",
    "        mlflow.log_artifact(importance_path)\n",
    "        os.remove(importance_path)  # Clean up temporary file\n",
    "\n",
    "        # Log model parameters as tags for easy filtering\n",
    "        mlflow.set_tag(\"model_type\", \"XGBoost\")\n",
    "        mlflow.set_tag(\"data_type\", \"embeddings\")\n",
    "\n",
    "        # Track best model based on F1 score\n",
    "        if metrics[\"f1\"] > best_f1_score:\n",
    "            best_f1_score = metrics[\"f1\"]\n",
    "            best_model = model\n",
    "            best_params = params\n",
    "            best_run_id = run.info.run_id\n",
    "            mlflow.set_tag(\"best_model\", \"True\")\n",
    "            print(f\"New best model found! F1 Score: {best_f1_score:.4f}\")\n",
    "        else:\n",
    "            mlflow.set_tag(\"best_model\", \"False\")\n",
    "\n",
    "    # 3. Store results for the final summary table\n",
    "    run_results = {\"run_name\": run_name, **params, **metrics}\n",
    "    results_list_xgb.append(run_results)\n",
    "\n",
    "print(\"\\n--- XGBoost Hyperparameter Search Complete ---\")\n",
    "print(f\"Best F1 Score: {best_f1_score:.4f}\")\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Model Run ID: {best_run_id}\")\n",
    "print(f\"MLflow tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"Experiment name: {experiment_name}\")\n",
    "print(\"To view results, run: mlflow ui\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c204df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Display the final results table ---\n",
    "print(\"\\n📈 XGBoost Results Summary Table:\\n\")\n",
    "results_df_xgb = pd.DataFrame(results_list_xgb).sort_values(by=\"f1\", ascending=False)\n",
    "\n",
    "# Create results directory\n",
    "results_dir = os.path.join(\"logs\", \"xgboost_results\")\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Save results summary\n",
    "results_path = os.path.join(results_dir, \"results_summary.csv\")\n",
    "results_df_xgb.to_csv(results_path, index=False)\n",
    "print(f\"Results saved to: {results_path}\")\n",
    "\n",
    "# Save the best model locally\n",
    "if best_model is not None:\n",
    "    import joblib\n",
    "\n",
    "    best_model_path = os.path.join(results_dir, \"best_xgboost_model.pkl\")\n",
    "    joblib.dump(best_model, best_model_path)\n",
    "    print(f\"Best model saved to: {best_model_path}\")\n",
    "\n",
    "    # Register the best model in MLflow Model Registry\n",
    "    model_name = \"XGBoost_Bug_Prediction_Embeddings\"\n",
    "    try:\n",
    "        # Register the model from the best run\n",
    "        model_uri = f\"runs:/{best_run_id}/model\"\n",
    "        mlflow.register_model(model_uri, model_name)\n",
    "        print(f\"Best model registered in MLflow Model Registry as: {model_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not register model in MLflow Model Registry: {e}\")\n",
    "\n",
    "    # Save best model info\n",
    "    best_model_info = {\n",
    "        \"best_f1_score\": best_f1_score,\n",
    "        \"best_params\": best_params,\n",
    "        \"best_run_id\": best_run_id,\n",
    "        \"model_path\": best_model_path,\n",
    "        \"timestamp\": pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    }\n",
    "\n",
    "    import json\n",
    "\n",
    "    best_info_path = os.path.join(results_dir, \"best_model_info.json\")\n",
    "    with open(best_info_path, \"w\") as f:\n",
    "        json.dump(best_model_info, f, indent=2)\n",
    "    print(f\"Best model info saved to: {best_info_path}\")\n",
    "\n",
    "print(f\"\\n🎯 Best Model Summary:\")\n",
    "print(f\"F1 Score: {best_f1_score:.4f}\")\n",
    "print(f\"Parameters: {best_params}\")\n",
    "print(f\"Run ID: {best_run_id}\")\n",
    "\n",
    "results_df_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c70ad99",
   "metadata": {},
   "source": [
    "## Best Model Usage Example\n",
    "\n",
    "The following cell demonstrates how to load and use the best model for predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7604c189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load and use the best model for predictions\n",
    "if best_model is not None:\n",
    "    # Option 1: Use the model that's already in memory\n",
    "    sample_predictions = best_model.predict(X_test[:5])\n",
    "    sample_probabilities = best_model.predict_proba(X_test[:5])\n",
    "\n",
    "    print(\"Sample predictions from best model:\")\n",
    "    print(f\"Predictions: {sample_predictions}\")\n",
    "    print(f\"Probabilities: {sample_probabilities}\")\n",
    "\n",
    "    # Option 2: Load the saved model from file\n",
    "    import joblib\n",
    "\n",
    "    loaded_model = joblib.load(os.path.join(results_dir, \"best_xgboost_model.pkl\"))\n",
    "    loaded_predictions = loaded_model.predict(X_test[:5])\n",
    "    print(f\"\\nVerification - Loaded model predictions: {loaded_predictions}\")\n",
    "    print(f\"Predictions match: {all(sample_predictions == loaded_predictions)}\")\n",
    "\n",
    "    # Option 3: Load model from MLflow\n",
    "    try:\n",
    "        model_uri = f\"runs:/{best_run_id}/model\"\n",
    "        mlflow_model = mlflow.xgboost.load_model(model_uri)\n",
    "        mlflow_predictions = mlflow_model.predict(X_test[:5])\n",
    "        print(f\"MLflow model predictions: {mlflow_predictions}\")\n",
    "        print(\n",
    "            f\"MLflow predictions match: {all(sample_predictions == mlflow_predictions)}\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load from MLflow: {e}\")\n",
    "else:\n",
    "    print(\"No best model available. Please run the training cells first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

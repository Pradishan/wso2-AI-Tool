{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0765a5a2",
   "metadata": {},
   "source": [
    "# Tokenizing small diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06beb156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import json\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12eb2b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "INPUT_CSV = \"small_diffs.csv\"\n",
    "TOKENIZED_DATA_DIR = \"tokenized_data_test/small_diffs\"\n",
    "CHUNK_SIZE = 500 # How many rows to process at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f7a1f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing small diffs: Resuming from chunk #0.\n"
     ]
    }
   ],
   "source": [
    "# --- Setup ---\n",
    "os.makedirs(TOKENIZED_DATA_DIR, exist_ok=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "# --- Checkpointing: Determine where to resume from ---\n",
    "processed_chunks = 0\n",
    "if os.path.exists(TOKENIZED_DATA_DIR):\n",
    "    existing_files = [f for f in os.listdir(TOKENIZED_DATA_DIR) if f.startswith('chunk_') and f.endswith('.pt')]\n",
    "    if existing_files:\n",
    "        processed_chunks = len(existing_files)\n",
    "\n",
    "start_chunk = processed_chunks\n",
    "rows_to_skip = start_chunk * CHUNK_SIZE\n",
    "print(f\"Processing small diffs: Resuming from chunk #{start_chunk}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f521fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e9b73f5e8764fbbb7448601396af592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Small Diffs: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Small diff processing complete. ✅\n"
     ]
    }
   ],
   "source": [
    "# --- Main Resumable Loop for Small Diffs ---\n",
    "try:\n",
    "    csv_reader = pd.read_csv(INPUT_CSV, chunksize=CHUNK_SIZE, skiprows=range(1, rows_to_skip + 1))\n",
    "    \n",
    "    total_rows_processed = 0\n",
    "    for i, chunk_df in enumerate(tqdm(csv_reader, desc=\"Processing Small Diffs\")):\n",
    "        current_chunk_index = start_chunk + i\n",
    "        \n",
    "        diff_texts = chunk_df['diff'].astype(str).tolist()\n",
    "        labels = chunk_df['is_bug_introducing'].tolist()\n",
    "        \n",
    "        encodings = tokenizer(diff_texts, truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n",
    "        \n",
    "        chunk_path = os.path.join(TOKENIZED_DATA_DIR, f\"chunk_{current_chunk_index}.pt\")\n",
    "        torch.save({\n",
    "            'input_ids': encodings['input_ids'],\n",
    "            'attention_mask': encodings['attention_mask'],\n",
    "            'labels': torch.tensor(labels)\n",
    "        }, chunk_path)\n",
    "        \n",
    "        total_rows_processed += len(chunk_df)\n",
    "        del diff_texts, labels, encodings, chunk_df\n",
    "        gc.collect()\n",
    "\n",
    "    print(f\"\\nSmall diff processing complete. ✅\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Input file not found at '{INPUT_CSV}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

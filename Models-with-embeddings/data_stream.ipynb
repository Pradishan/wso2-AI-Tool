{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "456f1a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087e413f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "LARGE_CSV_PATH = \"final_dataset_with_diffs.csv\"\n",
    "TOKENIZED_DATA_DIR = \"tokenized_data\" # A new directory to save the chunks\n",
    "CHUNK_SIZE = 100 # How many rows to process at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2f511a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pre-processing of 'final_dataset_with_diffs.csv'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adba26ca5a294b5e9e44bbae39f77fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Chunks: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Setup ---\n",
    "os.makedirs(TOKENIZED_DATA_DIR, exist_ok=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "total_rows = 0\n",
    "\n",
    "print(f\"Starting pre-processing of '{LARGE_CSV_PATH}'...\")\n",
    "# Create a data reader that yields chunks of the CSV\n",
    "csv_reader = pd.read_csv(LARGE_CSV_PATH, chunksize=CHUNK_SIZE)\n",
    "\n",
    "for i, chunk_df in enumerate(tqdm(csv_reader, desc=\"Processing Chunks\")):\n",
    "    diff_texts = chunk_df['diff'].astype(str).tolist()\n",
    "    labels = chunk_df['is_bug_introducing'].tolist()\n",
    "    \n",
    "    # Tokenize the batch of diffs\n",
    "    encodings = tokenizer(\n",
    "        diff_texts, \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", # Pad to a uniform length\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    # Prepare data as PyTorch tensors\n",
    "    input_ids = torch.tensor(encodings['input_ids'])\n",
    "    attention_mask = torch.tensor(encodings['attention_mask'])\n",
    "    labels_tensor = torch.tensor(labels)\n",
    "    \n",
    "    # Save the processed chunk to a file\n",
    "    chunk_path = os.path.join(TOKENIZED_DATA_DIR, f\"chunk_{i}.pt\")\n",
    "    torch.save({\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels_tensor\n",
    "    }, chunk_path)\n",
    "    \n",
    "    total_rows += len(chunk_df)\n",
    "\n",
    "# Save metadata about our dataset\n",
    "metadata = {'total_samples': total_rows, 'chunk_size': CHUNK_SIZE}\n",
    "with open(os.path.join(TOKENIZED_DATA_DIR, 'metadata.json'), 'w') as f:\n",
    "    json.dump(metadata, f)\n",
    "\n",
    "print(f\"\\nPre-processing complete. âœ… Saved {i+1} chunks to '{TOKENIZED_DATA_DIR}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
